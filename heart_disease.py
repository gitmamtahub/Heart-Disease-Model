# -*- coding: utf-8 -*-
"""Heart-disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lPHuoqqIpMvwe9brp4HXwj9XK92EaBAd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

df = pd.read_csv('heart_2020_cleaned.csv')
df.head()

df.info()

df.isnull().sum()

df=df.drop_duplicates() # drop from 319795 to 301717

df.shape

------------------------------EDA--------------------------------

df=df.dropna()

df.shape

df.describe()

df1=df # (Data Backup)

# How many people have Heart Disease?

labels = df['HeartDisease'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(df['HeartDisease'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Have Heart Disease')

HD=df[df['HeartDisease']=='Yes']
Male=HD[HD['Sex']=='Male']
Female=HD[HD['Sex']=='Female']
labels = HD['Sex'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(HD['Sex'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Gender Vs Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['Smoking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['Smoking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Smoking Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['Smoking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['Smoking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Smoking Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['AlcoholDrinking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['AlcoholDrinking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('AlcoholDrinking Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['AlcoholDrinking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['AlcoholDrinking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('AlcoholDrinking Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['Stroke'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['Stroke'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Stroke Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['Stroke'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['Stroke'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Stroke Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['DiffWalking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['DiffWalking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('DiffWalking Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['DiffWalking'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['DiffWalking'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('DiffWalking Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['PhysicalActivity'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['PhysicalActivity'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('PhysicalActivity Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['PhysicalActivity'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['PhysicalActivity'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('PhysicalActivity Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['Diabetic'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0,0, 0.1]

plt.pie(Male['Diabetic'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Diabetic Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['Diabetic'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0,0,0, 0.1]

plt.pie(Female['Diabetic'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Diabetic Vs Female Having Heart Disease')

Female['Diabetic'].value_counts()

Male['Diabetic'].value_counts()

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['Asthma'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['Asthma'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Asthma Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['Asthma'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['Asthma'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Asthma Vs Female Having Heart Disease')



fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['SkinCancer'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['SkinCancer'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('SkinCancer Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['SkinCancer'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['SkinCancer'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('SkinCancer Vs Female Having Heart Disease')

fig, axes = plt.subplots(1,2,figsize=(10,8))
plt.subplot(1,2,1)
labels = Male['KidneyDisease'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Male['KidneyDisease'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('kidneydisease Vs Male Having Heart Disease')

plt.subplot(1,2,2)
labels = Female['KidneyDisease'].value_counts().index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]

plt.pie(Female['KidneyDisease'].value_counts(), autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('KidneyDisease Vs Female Having Heart Disease')

df['Race'].value_counts()

fig, axes = plt.subplots(3,2,figsize=(11,8))
plt.subplot(3,2,1)
d=df[df['Race']=='White']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('White population Having Heart Disease')

plt.subplot(3,2,2)
d=df[df['Race']=='Black']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Black population Having Heart Disease')

plt.subplot(3,2,3)
d=df[df['Race']=='Hispanic']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Hispanic population Having Heart Disease')

plt.subplot(3,2,4)
d=df[df['Race']=='Other']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Other population Having Heart Disease')

plt.subplot(3,2,5)
d=df[df['Race']=='Asian']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('Asian population Having Heart Disease')

plt.subplot(3,2,6)
d=df[df['Race']=='American Indian/Alaskan Native']['HeartDisease'].value_counts()
labels=d.index
colors=['#9b5de5','#fee440']
explode=[0, 0.1]
plt.pie(d, autopct='%1.1f%%', labels=labels, colors=colors, explode=explode)
plt.title('American Indian/Alaskan Native population Having Heart Disease')

Male['GenHealth'].value_counts()

fig, axes = plt.subplots(1,2,figsize=(12,4))
plt.subplot(1,2,1)
sns.countplot(x='GenHealth', data=Male,order=['Poor','Fair','Good','Very good','Excellent'])
plt.title('GenHealth Vs Male Having heart Disease')

plt.subplot(1,2,2)
sns.countplot(x='GenHealth', data=Female,order=['Poor','Fair','Good','Very good','Excellent'])
plt.title('GenHealth Vs Female Having heart Disease')

fig, axes = plt.subplots(1,2,figsize=(12,4))
plt.subplot(1,2,1)
sns.countplot(x=sorted(Male['AgeCategory']))
plt.xticks(rotation=90)
plt.title('Age Vs Male Having heart Disease')

plt.subplot(1,2,2)
sns.countplot(x=sorted(Female['AgeCategory']))
plt.xticks(rotation=90)
plt.title('Age Vs Female Having heart Disease')

fig, axes = plt.subplots(1,2,figsize=(12,4))
plt.subplot(1,2,1)
sns.countplot(x=sorted(Male['SleepTime']))
plt.xticks(rotation=90)
plt.title('SleepTime Vs Male Having heart Disease')

plt.subplot(1,2,2)
sns.countplot(x=sorted(Female['SleepTime']))
plt.xticks(rotation=90)
plt.title('SleepTime Vs Female Having heart Disease')

df['BMI'].value_counts()

df['BMI'].describe()

data['BMI']=np.ceil(HD['BMI'])

data['BMI'].value_counts()



data['BMI'].describe()

data.describe()

bins=[12,15,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,95]
data['BMI']=pd.cut(data['BMI'],bins)

data['BMI'].value_counts()

BMI	WEIGHT STATUS
Below 18.5	Underweight
18.5 - 24.9	Healthy weight
25.0 - 29.9	Overweight
30.0 and above	Obesity

df['BMI'].loc[df['BMI']<18.5] = 0
df['BMI'].loc[(df['BMI']>=18.5) & (df['BMI']<25)]=1
df['BMI'].loc[(df['BMI']>=25) & (df['BMI']<30)]=2
df['BMI'].loc[df['BMI']>=30]=3
df['BMI'].head()

df['BMI'] = df.BMI.map({0:'Underweight',1:'Healthy',2:'Overweight',3:'Obesity'})

fig, axes = plt.subplots(1,2,figsize=(12,4))
plt.subplot(1,2,1)
sns.countplot(x=sorted(data['BMI']))
plt.xticks(rotation=90)
plt.title('BMI Index Vs Having HeartDisease')

plt.subplot(1,2,2)
HD1=df[df['HeartDisease']=='Yes']
sns.countplot(x=sorted(HD1['BMI']))
plt.xticks(rotation=90)
plt.title('BMI Weight Vs Having heart Disease')

df['PhysicalHealth'].value_counts()

HD['PhysicalHealth'].value_counts()

df['MentalHealth'].value_counts()

HD['MentalHealth'].value_counts()

#--------------------------------------Simple without feature selection technique used

df.nunique()

df = df[df.columns].replace({'Yes':1, 'No':0, 'Male':1, 'Female':0, 'No, borderline diabetes':0,'Yes (during pregnancy)':1, 'Y':1, 'N':0})
df.head()

df.head()

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

y=df['HeartDisease']
X=df.drop(['HeartDisease', 'AgeCategory', 'Race', 'GenHealth'], axis=1)

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

------------------------------------Simple with no numeric feature

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

y=df['HeartDisease']
X=df.drop(['HeartDisease', 'BMI', 'SleepTime', 'PhysicalHealth','MentalHealth'], axis=1)

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
col=X.columns
for c in col:
  if [X[c].dtype == 'object']:
    X[c]=le.fit_transform(X[c])

X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------With chi-sq

df.head()

from scipy import stats
df[['SleepTime','PhysicalHealth','MentalHealth','BMI']].corrwith(df['HeartDisease'].astype('float'), method=stats.pointbiserialr).round(2)

heatmap = df[['SleepTime','PhysicalHealth','MentalHealth','BMI']].corr().round(2)
fig = plt.figure(figsize=(8,6))
sns.heatmap(heatmap, vmin=-1, vmax=1, annot=True, cmap='coolwarm')

df2=df1

df2.head()

df2['BMI']=df['BMI']
df2.head()

df2.info()

pip install association-metrics

# Import association_metrics
import association_metrics as am
# Convert you str columns to Category columns
df2 = df2.apply(
        lambda x: x.astype("category") if x.dtype == "O" else x)

# Initialize a CamresV object using you pandas.DataFrame
cramersv = am.CramersV(df2)
# will return a pairwise matrix filled with Cramer's V, where columns and index are
# the categorical variables of the passed pandas.DataFrame
cramersv.fit()

heatmap = cramersv.fit().round(2)
fig = plt.figure(figsize=(8,6))
sns.heatmap(heatmap, vmin=-1, vmax=1, annot=True, cmap='coolwarm')

df2.info()

from sklearn.preprocessing import LabelEncoder, StandardScaler

col=df2.columns
le=LabelEncoder()
for c in col:
  if (df2[c].dtype == 'category'):
    df2[c]=le.fit_transform(df2[c])

df2.info()

X=df2.iloc[:,1:]
X.head()

X.shape # 13 features

Y=df2['HeartDisease']
Y.head()

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

test = SelectKBest(score_func=chi2, k=8)  # best 8 features
fit = test.fit(X, y)
fit.scores_

X_new=test.fit_transform(X, Y)

from sklearn.metrics import classification_report

#Split data
X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------------Selcting features with help of CramersV

df2.columns

X=df2[['Smoking', 'Stroke', 'DiffWalking', 'AgeCategory', 'Diabetic', 'GenHealth', 'KidneyDisease']]
X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

#from association_metrics.categorical import CramersV
-----------------------------feature selction with CramersV

X=df2[['Smoking', 'Stroke', 'DiffWalking', 'AgeCategory', 'Diabetic', 'GenHealth', 'KidneyDisease','PhysicalActivity', 'SkinCancer']]
X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

--------------------------------Balancing data

data=df1[df1['HeartDisease']=='Yes']
l=len(data)
new=df1[df1['HeartDisease']=='No']
sample=new.sample(l,random_state=0)
data=data.append(sample)

data.shape

l

data.info()

data.head()

data['HeartDisease'].value_counts()

data1=data

#--------------------------------------Simple without feature selection technique used

data.nunique()

data = data[data.columns].replace({'Yes':1, 'No':0, 'Male':1, 'Female':0, 'No, borderline diabetes':0,'Yes (during pregnancy)':1, 'Y':1, 'N':0})
data.head()

data.head()

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

y=data['HeartDisease']
X=data.drop(['HeartDisease', 'AgeCategory', 'Race', 'GenHealth'], axis=1)

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

------------------------------------Simple with no numeric feature

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

y=data['HeartDisease']
X=data.drop(['HeartDisease', 'BMI', 'SleepTime', 'PhysicalHealth','MentalHealth'], axis=1)

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
col=X.columns
for c in col:
  if [X[c].dtype == 'object']:
    X[c]=le.fit_transform(X[c])

X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------With chi-sq

from scipy import stats
corr_list = {}
y = df['HeartDisease'].astype(float)
for column in df:
    x = df[['SleepTime','PhysicalHealth','MentalHealth','BMI']].astype(float)
    corr = stats.pointbiserialr(x, y)
    corr_list[['SleepTime','PhysicalHealth','MentalHealth','BMI']] = corr
print(corr_list)

data.head()

from scipy import stats
data[['SleepTime','PhysicalHealth','MentalHealth','BMI']].corrwith(data['HeartDisease'].astype('float'), method=stats.pointbiserialr).round(2)

heatmap = df[['SleepTime','PhysicalHealth','MentalHealth','BMI']].corr().round(2)
fig = plt.figure(figsize=(8,6))
sns.heatmap(heatmap, vmin=-1, vmax=1, annot=True, cmap='coolwarm')

data2=data1

data2.head()

data2['BMI']=data['BMI']
data2.head()

data2.info()

pip install association-metrics

# Import association_metrics
import association_metrics as am
# Convert you str columns to Category columns
data2 = data2.apply(
        lambda x: x.astype("category") if x.dtype == "O" else x)

# Initialize a CamresV object using you pandas.DataFrame
cramersv = am.CramersV(data2)
# will return a pairwise matrix filled with Cramer's V, where columns and index are
# the categorical variables of the passed pandas.DataFrame
cramersv.fit()

heatmap = cramersv.fit().round(2)
fig = plt.figure(figsize=(8,6))
sns.heatmap(heatmap, vmin=-1, vmax=1, annot=True, cmap='coolwarm')

data2.info()

from sklearn.preprocessing import LabelEncoder, StandardScaler

col=data2.columns
le=LabelEncoder()
for c in col:
  if (data2[c].dtype == 'category'):
    data2[c]=le.fit_transform(data2[c])

data2.info()

X=data2.iloc[:,1:]
X.head()

X.shape # 13 features

Y=data2['HeartDisease']
Y.head()

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

test = SelectKBest(score_func=chi2, k=8)  # best 8 features
fit = test.fit(X, y)
fit.scores_

X_new=test.fit_transform(X, Y)

from sklearn.metrics import classification_report

#Split data
X_train, X_test, y_train, y_test = train_test_split(X_new, Y, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------------Selcting features with help of CramersV

data2.columns

X=data2[['Smoking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory', 'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth','KidneyDisease', 'SkinCancer']]
X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

#-------------------------------using SMOTE-------------------------------------------------------------------------------------------------

data=df
data = data[data.columns].replace({'Yes':1, 'No':0, 'Male':1, 'Female':0, 'No, borderline diabetes':0,'Yes (during pregnancy)':1, 'Y':1, 'N':0})
data.head()

data.info()

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
col=data.columns
for c in col:
  if [data[c].dtype == 'object']:
    data[c]=le.fit_transform(data[c])

features=data.iloc[:,1:]
target=data['HeartDisease']

from imblearn.over_sampling import SMOTE
from collections import Counter
smo=SMOTE(random_state=0,sampling_strategy=0.6)
x_re,y_re=smo.fit_resample(features,target)
print(sorted(Counter(y_re).items()))

data2=df
data2.head()

features.shape,target.shape,x_re.shape,y_re.shape

x_re.info()

features.info()

data.info()

data.head()

y_re.value_counts()

x_re=x_re.drop(columns=['BMI'])

x_re.shape

#--------------------------------------Simple without feature selection technique used

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split
y=y_re
X=x_re.drop(['AgeCategory', 'Race', 'GenHealth'], axis=1)

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

------------------------------------Simple with no numeric feature

#Split dataset for training and testing
from sklearn.linear_model import LogisticRegression
from sklearn import tree
from sklearn.model_selection import train_test_split

y=y_re
X=x_re.drop(['SleepTime', 'PhysicalHealth','MentalHealth'], axis=1)

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred = rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred = xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------With chi-sq

from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

test = SelectKBest(score_func=chi2, k=8)  # best 8 features
fit = test.fit(x_re, y_re)
fit.scores_

X_new=test.fit_transform(x_re, y_re)

from sklearn.metrics import classification_report

#Split data
X_train, X_test, y_train, y_test = train_test_split(X_new, y_re, test_size=0.2, random_state=0)

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

---------------------------------Selcting features with help of CramersV

X=x_re[['Smoking', 'Stroke', 'DiffWalking', 'Sex', 'AgeCategory', 'Race', 'Diabetic', 'PhysicalActivity', 'GenHealth','KidneyDisease', 'SkinCancer']]
X.head()

#Split data
X_train, X_test, y_train, y_test = train_test_split(X, y_re, test_size=0.2, random_state=0)

X_train.shape,X_test.shape

log_reg = LogisticRegression(max_iter=200000)
log_reg.fit(X_train,y_train)

#Check the accuracy on the training set

from sklearn.metrics import accuracy_score, confusion_matrix

y_pred=log_reg.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.tree import DecisionTreeClassifier

clf = DecisionTreeClassifier(random_state=42)
clf.fit(X_train, y_train)

y_pred=clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from sklearn.ensemble import RandomForestClassifier

rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

y_pred=rf_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)

from xgboost import XGBClassifier

# Agora vamos repetir o processo para o XGBClassifier
xgb_clf = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb_clf.fit(X_train, y_train)

y_pred=xgb_clf.predict(X_test)
cm=confusion_matrix(y_test,y_pred)
rep=classification_report(y_test,y_pred)
print("confusion Matric :\n",cm)
print("Classification Report :\n",rep)